{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as px\n",
    "from utils.transcription_utils import get_transcription_info,get_stats,get_total_stats\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=Path(\"/media/dataset-harddisk/munikumar/utils/audio_utils/data/mucs/normalize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir=Path(\"/media/dataset-harddisk/munikumar/hindi_dataset/mucs/mucs_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcription(transcription_path):\n",
    "\n",
    "    with open(transcription_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "        data = fp.read().splitlines()\n",
    "        transcription_data = {\n",
    "            transcription.split(\" \")[0] + \".wav\": \" \".join(transcription.split(\" \")[1:])\n",
    "            for transcription in data\n",
    "        }\n",
    "    \n",
    "    return transcription_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting audio info about : normalize - test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3897 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3897/3897 [04:13<00:00, 15.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Loss of audio data without transcription normalization:\n",
      "**************************************************\n",
      "Total duration : 5:29:26\n",
      "Loss duration : 0:0:0\n",
      "Remaining duration : 5:29:26\n",
      "Drop by percentage: 0.00\n",
      "\n",
      "Total audio files : 3897\n",
      "Files that would be drop: 0\n",
      "Remaining audio files : 3897\n",
      "Drop by percentage: 0.00\n",
      "**************************************************\n",
      "Non hindi tokes : []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_transcription_data=load_transcription(dataset_dir/\"subtask1_blindtest_wReadme\"/\"Hindi\"/\"transcription.txt\")\n",
    "get_transcription_info(test_transcription_data,dataset_dir/\"subtask1_blindtest_wReadme\"/\"Hindi\"/\"audio\",save_path,\"test\")\n",
    "get_stats(save_path/\"test_non_hindi_transcripts.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset\n",
    "**************************************************\n",
    "**Loss of audio data without transcription normalization:**\n",
    "**************************************************\n",
    "Total duration : 5:29:26  \n",
    "Loss duration : 1:54:17  \n",
    "Remaining duration : 3:35:9  \n",
    "Drop by percentage: 34.69  \n",
    "\n",
    "Total audio files : 3897  \n",
    "Files that would be drop: 1346  \n",
    "Remaining audio files : 2551  \n",
    "Drop by percentage: 34.54  \n",
    "**************************************************\n",
    "Non hindi tokes : ['ग़', 'ज़', 'ड़', 'ढ़', 'फ़']\n",
    "\n",
    "**************************************************\n",
    "**Loss of audio data After decomposing nuktas:**\n",
    "**************************************************\n",
    "Total duration : 5:29:26  \n",
    "Loss duration : 0:0:0  \n",
    "Remaining duration : 5:29:26  \n",
    "Drop by percentage: 0.00  \n",
    "\n",
    "Total audio files : 3897  \n",
    "Files that would be drop: 0  \n",
    "Remaining audio files : 3897  \n",
    "Drop by percentage: 0.00  \n",
    "**************************************************\n",
    "Non hindi tokes : []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting audio info about : normalize - dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3843 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3843/3843 [04:18<00:00, 14.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Loss of audio data without transcription normalization:\n",
      "**************************************************\n",
      "Total duration : 5:33:25\n",
      "Loss duration : 0:0:48\n",
      "Remaining duration : 5:32:37\n",
      "Drop by percentage: 0.24\n",
      "\n",
      "Total audio files : 3843\n",
      "Files that would be drop: 10\n",
      "Remaining audio files : 3833\n",
      "Drop by percentage: 0.26\n",
      "**************************************************\n",
      "Non hindi tokes : ['ऍ']\n"
     ]
    }
   ],
   "source": [
    "dev_transcription_data=load_transcription(dataset_dir/\"test\"/\"transcription.txt\")\n",
    "get_transcription_info(dev_transcription_data,dataset_dir/\"test\"/\"audio\",save_path,\"dev\")\n",
    "get_stats(save_path/\"dev_non_hindi_transcripts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev Dataset\n",
    "**************************************************\n",
    "**Loss of audio data without transcription normalization:**\n",
    "**************************************************\n",
    "Total duration : 5:33:25  \n",
    "Loss duration : 1:11:37  \n",
    "Remaining duration : 4:21:47  \n",
    "Drop by percentage: 21.48  \n",
    "\n",
    "Total audio files : 3843  \n",
    "Files that would be drop: 781  \n",
    "Remaining audio files : 3062  \n",
    "Drop by percentage: 20.32  \n",
    "**************************************************\n",
    "Non hindi tokes : ['ऍ', 'क़', 'ख़', 'ज़', 'ड़', 'ढ़', 'फ़']\n",
    "\n",
    "**************************************************\n",
    "**Loss of audio data After Decomposing Nuktas:**\n",
    "**************************************************\n",
    "Total duration : 5:33:25  \n",
    "Loss duration : 0:0:48  \n",
    "Remaining duration : 5:32:37  \n",
    "Drop by percentage: 0.24  \n",
    "\n",
    "Total audio files : 3843  \n",
    "Files that would be drop: 10  \n",
    "Remaining audio files : 3833  \n",
    "Drop by percentage: 0.26  \n",
    "**************************************************\n",
    "Non hindi tokes : ['ऍ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting audio info about : normalize - train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 25536/99925 [33:03<1:16:48, 16.14it/s]"
     ]
    }
   ],
   "source": [
    "train_transcription_data=load_transcription(dataset_dir/\"train\"/\"transcription.txt\")\n",
    "get_transcription_info(train_transcription_data,dataset_dir/\"train\"/\"audio\",save_path,\"train\")\n",
    "get_stats(save_path/\"train_non_hindi_transcripts.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Dataset:\n",
    "**************************************************\n",
    "Loss of audio data without transcription normalization:\n",
    "**************************************************\n",
    "Total duration : 95:3:0  \n",
    "Loss duration : 11:36:50  \n",
    "Remaining duration : 83:26:9  \n",
    "Drop by percentage: 12.22  \n",
    "\n",
    "Total audio files : 99925  \n",
    "Files that would be drop: 10851  \n",
    "Remaining audio files : 89074  \n",
    "Drop by percentage: 10.86  \n",
    "**************************************************\n",
    "Non hindi tokes : ['ऍ', 'क़', 'ख़', 'ग़', 'ज़', 'ड़', 'ढ़', 'फ़', 'ॠ']\n",
    "\n",
    "**************************************************\n",
    "Loss of audio data After Decomposing Nuktas:\n",
    "**************************************************\n",
    "Total duration : 95:3:0  \n",
    "Loss duration : 0:18:26  \n",
    "Remaining duration : 94:44:33  \n",
    "Drop by percentage: 0.32  \n",
    "\n",
    "Total audio files : 99925  \n",
    "Files that would be drop: 268  \n",
    "Remaining audio files : 99657  \n",
    "Drop by percentage: 0.27  \n",
    "**************************************************\n",
    "Non hindi tokes : ['ऍ', 'ॠ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Loss of audio data for total dataset:\n",
      "**************************************************\n",
      "Total duration : 106:5:51\n",
      "Loss duration : 0:19:14\n",
      "Remaining duration : 105:46:37\n",
      "Drop by percentage: 0.30\n",
      "\n",
      "Total audio files : 107665\n",
      "Files that would be drop: 278\n",
      "Remaining audio files : 107387\n",
      "Drop by percentage: 0.26\n"
     ]
    }
   ],
   "source": [
    "get_total_stats(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Dataset\n",
    "**************************************************\n",
    "Loss of audio data for total dataset:\n",
    "**************************************************\n",
    "Total duration : 106:5:51  \n",
    "Loss duration : 14:42:45  \n",
    "Remaining duration : 91:23:6  \n",
    "Drop by percentage: 13.87  \n",
    "\n",
    "Total audio files : 107665  \n",
    "Files that would be drop: 12978  \n",
    "Remaining audio files : 94687  \n",
    "Drop by percentage: 12.05  \n",
    "\n",
    "**************************************************\n",
    "Loss of audio data for total dataset After decomposing Nuktas:\n",
    "**************************************************\n",
    "Total duration : 106:5:51  \n",
    "Loss duration : 0:19:14  \n",
    "Remaining duration : 105:46:37  \n",
    "Drop by percentage: 0.30  \n",
    "\n",
    "Total audio files : 107665  \n",
    "Files that would be drop: 278  \n",
    "Remaining audio files : 107387  \n",
    "Drop by percentage: 0.26  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw audio duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_raw audio duration : 95:03:00  \n",
    "dev_raw audio duration: 5:33:25  \n",
    "test_raw audio duration: 5:29:26  \n",
    "total_duration: 106:05:51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total number of audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_audio_files: 99925  \n",
    "dev_audio_files: 3843  \n",
    "test_audio_files: 3897  \n",
    "total_audio files: 107665\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
